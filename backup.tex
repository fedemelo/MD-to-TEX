% !TeX spellcheck = es_ES
\documentclass{fmbnotes}
\usepackage{fmbmath}

\begin{document}

\newcommand*{\titulo}{test}
\portada{\titulo} 

\begin{center}
    {\Large\bfseries\sffamily \titulo}
\end{center}

Disciplina de la IA que busca dotar a las computadoras con la capacidad de entender el lenguaje natural (el de los humanos).
El curso se enfoca en la parte escrita.

\level{1}{Introducción al Procesamiento de lenguaje natural}
\label{sec:introduccion_al_procesamiento_de_lenguaje_natural}

\level{2}{Qué es un lenguaje (natural)}
\label{sec:que_es_un_lenguaje_natural_}

Un lenguaje humano es un sistema convencional basado en el uso de signos / palabras según un sistema de reglas. 

\level{3}{Elementos de un lenguaje}
\label{sec:elementos_de_un_lenguaje}
1. \textbf{Alfabeto}: Conjunto de \textbf{lexemas} (caracteres), símbolos lexicográficos.
		En ocasiones, los lexemas son unidades de significado completo (pasa mucho en lenguajes asiáticos, por los kanjis)
2. \textbf{Gramática}:
	\textbf{Morfología}: Estudia la raíz de las palabras. De dónde vienen. 
		\textbf{Palabra}: Conjunto ordenado de lexemas.
	\textbf{Sintaxis}: Reglas para la posición de las palabras.
3. \textbf{Semántica}: Significado de una palabra.
4. \textbf{Fonética}: Cómo se pronuncian las palabras.

\level{3}{¿Por qué es difícil entender el lenguaje natural?}
\label{sec:_por_que_es_dificil_entender_el_lenguaje_natural_}
Los lenguajes de máquina son determinísticos. Después de un `def` va un nombre.
Los naturales no; tienen \textbf{ambigüedad}:
1. \textbf{Ambigüedad léxico-emántica} (\textbf{Polisemia}): Una misma palabra tiene el mismo significado.
2. \textbf{Ambigüedad sintáctica} (\textbf{Anfibología})
	"Vimos a Pedro tomando clase"
	1. Mientras tomábamos clase, vimos a Pedro.
	2. Vimos a Pedro, que estaba tomando clase.
3. \textbf{Ambigüedad fonética}: Me diste (de dar), mediste (medir).
	"Ya tú viste Redes?"
	"Ya tuviste Redes?"

Tienen \textbf{correferencia}: es complicado saber a qué nombre hace referencia un pronombre.
"La \textit{lámpara} no cabe por la \textit{puerta} porque \textit{esta} es muy \textit{angosta}" ¿Qué es angosto? La puerta (si fuera la lámpara, cabría).
"La \textit{lámpara} no cabe por la \textit{puerta} porque \textit{esta} es muy \textit{alta}" ¿Qué es alto? Pues la lámpara (si fuera la puerta, cabría)
Para un humano, él entiende, pero porque tiene entendimiento del mundo.
Pero para la máquina, ¿cómo puede saber?

Existe \textbf{sarcasmo} e \textbf{ironía}. La semántica es opuesta a lo que se dice.
"Me gusta su perfume, ¿cuánto tiempo ha pasado sumergido en él?"

También: lenguaje no estándar (redes sociales), modismos, neologismos, nombres de entidades compuestos (en lenguas derivadas del latín, que hay sustantivos compuestos, e.g. Música Ligera).

Es por eso que los métodos que se aplican a imágenes no se pueden pasar a texto.

\level{2}{Soluciones para NLP}
\label{sec:soluciones_para_nlp}

\level{3}{Consolidadas}
\label{sec:consolidadas}
Cosas que ya sabemos resolver bien, ya hay una librería que lo hace. 

1. \textbf{Clasificación de texto}. 
	1. Clasificación binaria. E.g: Es spam o no?
	2. \textbf{Análisis de polaridad}. Esto es positivo o es negativo?
2. Identificación de partes de la oración (\textbf{POS}). Es un verbo? Un sustantivo?
3. Identificación de entidades nombradas (\textbf{NER}, \textit{Named Entities}). Quiero saber si algo es un nombre, una entidad, una organización.
4. Recuperación de información. Buscadores como google, bing. Envío un query y recupero documentos pertinentes. Es fácil; lo difícil es que sea eficiente.

\level{3}{Gran progreso}
\label{sec:gran_progreso}

\textbf{Correferencia}: Reconocer a qué sustantivo refiere un pronombre. Relación entre dos o más elementos lingüísticos cuyo referente es común

\textbf{Desambiguación}: Corregir la ambigüedad: significado de una palabra en un contexto particular.

\textbf{Traducción}: Entra una secuencia de un lenguaje, busco generar una secuencia en otro lenguaje que preserve el significado semántico.

\level{3}{Desafíos}
\label{sec:desafios}
No existe una solución consolidada.

\textbf{Q&A}: Respuesta a preguntas, que sean genéricas, de cualquier tema.

\textbf{Chatbot}: Se espera una interacción humana: que use contexto, que recuerde qué ha dicho antes, que te devuelva una pregunta.

\textbf{Resúmenes automáticos}: Sintetizar un texto en un conjunto de sentencias de menor tamaño. "Necesito un resumen del x%, no refrasee, conserve las citas, etc"
\level{3}{Enfoques para soluciones de NLP}
\label{sec:enfoques_para_soluciones_de_nlp}
\level{4}{Enfoque clásico: basado en reglas}
\label{sec:enfoque_clasico_basado_en_reglas}

Ejemplo: etiquetador de partes de la oración.

1. Formalizo la sintaxis con producciones, hago una gramática.
	Sé que 
	oración -  sintagma nominal  sintagma verbal
	sintagma nominal - ...
2. Recibo una entrada, que sé que sigue esas reglas.
3. El programa trata de etiquetar la entrada con base en las reglas.

Ventajas:
+\textbf{Salida explicable y auditable}: Sé exactamente qué hizo el modelo para dar su respuesta.
Desventajas:
-\textit{Requieren expertos}: Alguien tiene que diseñar las reglas.
-\textit{Extenuante}: Las reglas tienen que contemplar \textit{todas} las posibilidades. Se complejizan.

\level{4}{Enfoque basado en datos: Machine Learning}
\label{sec:enfoque_basado_en_datos_machine_learning}

El cerebro humano es un \textbf{clasificador discriminativo}.

Desventajas:
-\textit{No son explicables}: Es muy complicado saber por qué genero tal o cual salida.
-\textit{Está preparado}: Complicado saber si está preparado para todas las posibles entradas, puede que bote algo paila.
-\textit{Volumen de datos}: Requiere muchos. (\()
-**Datos de calidad**: Necesario remover el ruido y preprocesarlos. (\))
Ventajas:
+Son mejores.


\level{1}{Recuperación de información}
\label{sec:recuperacion_de_informacion}
La IR (\textit{Information Recovery}) es encontrar material (usualmente documentos) no estucturados (usualmente texto) que satisface una búsqueda de información (un query) dentro de grandes colecciones (usualmente en computadores).

Componentes:

\begin{itemize}
    \item \textbf{Colección}: conjunto (estático, por ahora) de documentos.
    \item \textbf{Objetivo}: encontrar info útil.
\end{itemize}

Pasos:
1. \textbf{Procesamiento de texto}: Quiero el texto en bruto, quito imágenes, etiquetas en un HTML, comentarios en un código.
2. \textbf{Representación}: Necesito representar el texto como un objeto matemático, usualmente un vector. Podría solo coger el binario detrás pero no me sirve de mucho.
3. \textbf{Modelamiento}: Monto el modelo.

\level{2}{Procesamiento de texto}
\label{sec:procesamiento_de_texto}
La idea es procesar las palabras hasta obtener un vocabulario de tokens.

\level{3}{1. Tokenizar}
\label{sec:1_tokenizar}
\textbf{Tokenizar}: Dividir el texto en las unidades mínimas de procesamiento.

Usualmente, los tokens son palabras. Hay tokenizadores más pequeños.

\textbf{Desafíos}:


\begin{itemize}
    \item Apostrofes en el inglés (usadas para posesivo y para contracción). "Joe's". Hago dos tokens?
    \item Palabras compuestas: Hewlett-ackard, Música Ligera. De nuevo, dos tokens?
    \item Alemán: palabras compuestas de verdad.
    \item Tokenizar lenguas asiáticas. Japonés: Mezclan alfabetos!Chino: Cada caracter tiene significado completo!Era hasta mejor traducir a otra lengua antes y después de tokenizar.
\end{itemize}

Evidente entonces que depende del idioma. Así que tokenizar está de la mano con el reconocimiento de lenguaje.

Esto se hace siempre o casi siempre.

\level{3}{1.5. (Opcional) Eliminación de palabras de parada}
\label{sec:1_5_opcional_eliminacion_de_palabras_de_parada}

\textbf{Palabras de parada}: Palabras que carecen de un significado semántico.
No aportan semánticamente (sintácticamente sí) en la generación de sentencias.

En español: los \textbf{determinantes} suelen ser estructurales:

\begin{itemize}
    \item Artículos: el, las, lo, los...
    \item Demostrativos: este, esta, estos, estas, aquel...
    \item Indefinidos: una, uno, algunos, muchos,
    \item Posesivos:
    \item Numerales::
    \item Interrogativos
    \item Exclamativos:
\end{itemize}
Vale la pena removerlos solo en ciertas aplicaciones. De resto, y actualmente, no.

\level{3}{2. Normalizar}
\label{sec:2_normalizar}

Hay palabras que yo querré tomar como el mismo token.

"Rubén", "ruben", "ruBen". "USA" y "u.s.a". Todas tienen el mismo significado semántico, yo querré que sean el mismo token.

Así que las pongo en forma estándar: mato acentos, mayúsculas.

Con eso quedo con \textbf{términos}: palabras normalizadas que pondré en el diccionario del sistema de IR.


\level{3}{3. Lematización}
\label{sec:3_lematizacion}

Tomo las formas flexivas/variantes a la forma base.

Conozco esas formas con los estudios de morfología. Los lingüistas estudian y se arman un lematizador. (aunque ojo porque la morfología no es nada sencilla. De hecho para español no existe aún un buen lematizador).

E.g:
pan: panadero, panadería, panecillo.
Problema: se pierde bastante semántica.. aunque en IR, para algunos lenguajes, no pierde mucha precisión.

\level{3}{3. (Alternativo) Stemming}
\label{sec:3_alternativo_stemming}

Lematizar es difícil, de forma que una idea es cortar las palabras, con reglas arbitrarias.

No tiene base teórica lingüística, pero al parecer funciona y reduce el lenguaje.
Como para los lingüistas esto debe ser senda chambonada, las reglas las diseñaron ingenieros a la maldita sea.
Básicamente: mochan las terminaciones más comunes, "-ión" y todo eso. 

"Automatizar", "automático", "automatización" - "automat".

Se hace con expresiones regulares (que es la forma más eficiente de búsqueda en texto).


\level{2}{Representación}
\label{sec:representacion}

Se usan matrices término-ocumento.

\level{3}{Matriz término-ocumento binaria}
\label{sec:matriz_termino_ocumento_binaria}
Cada documento es representado por un vector binario, \(v \in \{0,1\}^{|V|}\).

Tengo una super matriz donde cada fila representa un token, cada columna representa un documento, un 1 representa que el token está en el documento y 0 que no.

Con eso, para saber si una palabra está y otra no y así, hago operaciones lógicas entre los vectores a nivel de bits.
+Eso es bien eficiente, mientras los vectores sean chiquitos, el tamaño de la matriz sea decente.
-o hay donde guardarla y encima esas matrices suelen ser muuy dispersas, de forma que peor.

\level{3}{Índice invertido}
\label{sec:indice_invertido}
Sabiendo que la matriz era dispersa y por ende ineficiente, se inventaron otra estructura.

(Realmente, los genios se dieron cuenta de lo mismo que con grafos: si tengo un grafo disperso, naturalmente lo represento con una lista de adyacencias, no una matriz de adyacencias).

Para cada token, la lista de documentos que lo contienen, ordenados (por algún orden arbitrario). El orden es para que se puedan hacer operaciones binarias.

\textbf{Diccionario}: Conjunto de tokens.
\textbf{Postings}: Conjunto de documentos en los que se encuentra un token.

Dada la lista de tokens, cada uno con el ID del documento en el que aparece:

\begin{itemize}
    \item Se ordenan los tokens (de lo contrario el algoritmo no sería polinomial)
    \item Se arma el índice invertido recorriendo toda la lista.
\end{itemize}
Al posting se le añadirá una longitud, el número de documentos que están en el posting. Eso se denominará \textbf{frecuencia documental}: cuantas veces aparece en todos los documentos es token.


Linux: `GREP`

\level{2}{Modelamiento}
\label{sec:modelamiento}

\level{3}{Modelo de recuperación booleana}
\label{sec:modelo_de_recuperacion_booleana}


\begin{itemize}
    \item Cada documento se toma como un conjunto (sin orden, sin repetidos) de palabras.
    \item O el documento cumple la condición o no la cumple.
    \item Modelo más simple de recuperación de la información.
\end{itemize}

\level{4}{Consultas \textit{and}}
\label{sec:consultas_textit_and_}

Se busca A y B.
1. Tomar la lista de postings de A de la tabla de hash, O(1). 
2. Tomar la lista de postings de A de la tabla de hash, O(1). 
3. Se usa el algoritmo de \textit{merge} para una lista ordenada de los postings de A y B. 

\textbf{Algoritmo de merge}:

Comienzo por el primer elemento de cada lista.
Reviso si los dos elementos que estoy comparando son iguales. 

\begin{itemize}
    \item Si sí, paso al siguiente elemento en ambas listas.
    \item Si no, paso al siguiente elemento únicamente en la lista del elemento que al comparar fue menor.
\end{itemize}

Sean \(x\) y \(y\) las longitudes de las listas, \(O(x+y)\).

\level{4}{A \textit{and not} b}
\label{sec:a_textit_and_not_b}

Muy similar, pero cambia el algoritmo de merge:

Comienzo por el primer elemento de cada lista.
Reviso si los dos elementos que estoy comparando son iguales. 

\begin{itemize}
    \item Si sí, paso al siguiente elemento en ambas listas.
    \item Si no, paso al siguiente elemento únicamente en la lista del elemento que al comparar fue menor.
\end{itemize}

Sean \(x\) y \(y\) las longitudes de las listas, \(O(x+y)\).

\level{4}{A \textit{or not} B}
\label{sec:a_textit_or_not_b}

Que o tenga A o no tenga B.
Me sirve que:

\begin{itemize}
    \item Tenga A.
    \item No tenga B (o no tenga ninguno).
\end{itemize}

Entonces esa condición técnicamente la cumplen la gran mayoría de documentos. 
Sin embargo, esto es una \textbf{búsqueda}. La gente busca las palabras que conoce. Entonces, me limito a hacer la búsqueda en los postings de A y B, es decir:  en \textbf{A or B}.

\level{4}{Optimizar consultas}
\label{sec:optimizar_consultas}

1. Caso fácil, solo \textbf{ands}. Busco las palabras en orden de menor a mayor frecuencia documental, desde la que en menos docs aparecen.
2. (A or B) and (C or D). Empiezo 

%TODO
Limitaciones y demás diapos

\level{3}{Modelos de recuperación ranqueada}
\label{sec:modelos_de_recuperacion_ranqueada}

El sistema devuelve un orden de los documentos más relevantes de la colección, los \(n\) primeros.


\begin{itemize}
    \item El tamaño del set resultante no es problema.
    \item Se muestran los top \(n\) resultados.
    \item No se da al usuario \textit{overwhelming} resultados.
\end{itemize}

+No estamos limitados a consultas binarias.

\textbf{Coeficiente de Jaccard}: Forma primitiva de dar similitud entre conjuntos. 
\begin{equation*}
    \text{jaccard}(A, B) = |A \cap B|/|A\cup B|
\end{equation*}
Problema: Si un conjunto es mucho más grande que otro, va a afectar el resultado, va a dar un resultado engañoso.

\level{4}{BOW: Modelo de Bolsa de Palabras}
\label{sec:bow_modelo_de_bolsa_de_palabras}

Se construye una matriz término-ocumento.

-a representación vectorial no considera el orden de las palabras en el documento.
Eso hace que no se preserve la semántica:
"A es más rápido que B"
"B es más rápido que A"
Ambas tienen la misma representación.

La frecuencia del término \(t\) en el documento \(d\) s define como \(tf_{t,d}\).
No se puede usar puramente la frecuencia, no es proporcional a la relevancia (e.g. solo un texto con una palabra repetida a la maldita sea).

Si \(tf_{t,d} > 0\), se calcula un peso como \(w_{t,d} = 1+\log_{10}tf_{t,d}\).
El logaritmo es para que si hay muchísimas apariciones, el peso no crezca tanto. El +1 es porque si hay una sola aparición, el logaritmo daría 0.

Gran problema del enfoque frecuentista: digamos que hago un query con palabras muy comunes ("agua", "dormir") y una palabra rara pero muy importante (el nombre de la enfermedad).
El único documento que tiene la palabra rara quedaría excluido de la búsqueda bajo un enfoque puramente frecuentista.

\level{4}{Discriminancia}
\label{sec:discriminancia}

¿Cómo le doy más importancia a los términos raros?

Piense que si \(df_t\) (frecuencia documental del término \(t\)) aparece en todos lados, entonces es una palabra irrelevante (e.g. palabras de parada). Su poder discriminativo es casi nulo.

Si un término solo aparece una vez en un documento, tendrá un alto poder discriminatorio. 

Se define entonces la \textbf{frecuencia inversa documental} del término \(t\) como 
\begin{equation*}
    \operatorname{idf}_t = \log_{10} \cdot \frac{N}{\operatorname{df}_t}
\end{equation*}
con eso, las palabras de parada no tienen ningún peso. Se removían entonces para disminuir el tamaño del corpus documental.

\level{4}{TF-DF: \textit{Term Frequency -Inverse Document Frequency}}
\label{sec:tf_df_textit_term_frequency_inverse_document_frequency_}

Se llega entonces al producto de qué tan frecuente es la palabra en el documento y qué tan única es la palabra en el corpus documental. 


Con eso, pongo de primero el documento con mayor score, donde 
\begin{equation*}
    Score(q,d) = \sum_{}
\end{equation*}

%TODO
\level{3}{Documentos como vectores}
\label{sec:documentos_como_vectores}
Avance de lo anterior, la idea es tratar de preservar la semántica.

\level{4}{Representación vectorial con base en tf-df}
\label{sec:representacion_vectorial_con_base_en_tf_df}
Primero, se pensó en tomar la matriz pero en lugar qde una matriz de frecuencia, con ponderaciones tf-df.

Tiene los mismos problemas del vector binario.

Se utiliza la distancia GOSSIP

\begin{itemize}
    \item Yo quiero el ángulo entre el vector de query y los documentos. Quiero el menor.
    \item Hago productos puntos. Despejo la fórmula del producto punto y de ahí saco un coseno.
\end{itemize}
%TODO:
Fórmula


\level{1}{Modelos de lenguaje N-ramas}
\label{sec:modelos_de_lenguaje_n_ramas}
\level{1}{Clasificación de texto (regresión logística)}
\label{sec:clasificacion_de_texto_regresion_logistica_}
\level{1}{Incrustaciones de palabras}
\label{sec:incrustaciones_de_palabras}
\level{1}{Redes Neuronales}
\label{sec:redes_neuronales}
\level{1}{Incrustaciones contextuales}
\label{sec:incrustaciones_contextuales}
\level{1}{Grandes modelos de lenguajes -aplicaciones}
\label{sec:grandes_modelos_de_lenguajes_aplicaciones}
\end{document}